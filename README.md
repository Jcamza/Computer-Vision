# Computer-Vision-paper
As a new postgraduate, this repository contains a collection of papers on computer vision and artificial intelligence to facilitate my learning and document my progress.
My advisor's personal [homepage](https://kdd-code.github.io/).

 [2025-ICLR] **Near, Far: Patch-Ordering Enhances Vision Foundation Models’ Scene Understanding**[[paper](https://arxiv.org/pdf/2408.11054)][[code](https://github.com/vpariza/NeCo)]  

### Model Selection
 [2024-NeurIPS] **Bridge the Modality and Capability Gaps in Vision-Language Model Selection**[[paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/3d007df4ae13adf9001f8969555b11bd-Paper-Conference.pdf)][[code](https://github.com/YCaigogogo/SWAB)]  
 [2024-Arxiv] **Learning to Rank Pre-trained Vision-Language Models for Downstream Tasks**[[paper](https://arxiv.org/pdf/2412.20682)]  
 
 [2025-IJCAI] **AUnifying Perspective on Model Reuse: From Small to Large Pre-Trained Models**[[[paper](https://www.ijcai.org/proceedings/2025/1201.pdf)][[code](https://github.com/LAMDA-Model-Reuse/Awesome-Model-Reuse)]  
 [2025-ICCV] **Consensus-Driven Active Model Selection**[[paper](https://openaccess.thecvf.com/content/ICCV2025/papers/Kay_Consensus-Driven_Active_Model_Selection_ICCV_2025_paper.pdf)][[code](https://github.com/justinkay/coda)]  
 [2025-Arxiv] **ONE-EMBEDDING-FITS-ALL: EFFICIENT ZERO-SHOT TIME SERIES FORECASTING BY A MODEL ZOO**[[paper](https://arxiv.org/pdf/2509.04208?)]  
 [2025-Arxiv] **Generalizing Vision-Language Models to Novel Domains: A Comprehensive Survey**[[paper](https://arxiv.org/pdf/2506.18504)]  
 [2025-Arxiv] **Vision-Language Model Selection and Reuse for Downstream Adaptation**[[paper](https://arxiv.org/pdf/2501.18271)]  
 

### Self-Supervised Learning（Including clustering）
 [2023-BMVC] **TEMI: Exploring the Limits of Deep Image Clustering using Pretrained Models**[[paper](https://arxiv.org/pdf/2303.17896)][[code](https://github.com/HHU-MMBS/TEMI-official-BMVC2023)]  
 [2023-ICCV] **Zero-Shot Composed Image Retrieval with Textual Inversion**[[paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Baldrati_Zero-Shot_Composed_Image_Retrieval_with_Textual_Inversion_ICCV_2023_paper.pdf)][[code](https://github.com/miccunifi/SEARLE)]  
 
 [2024-Arxiv] **DPA: Dual Prototypes Alignment for Unsupervised Adaptation of Vision-Language Models**[[paper](https://arxiv.org/pdf/2408.08855)][[code(暂无)]]  
 [2024-Arxiv] **Let Go of Your Labels with Unsupervised Transfer**[[paper](https://arxiv.org/pdf/2406.07236)][[code]()]  
 [2024-Arxiv] **Vocabulary-free Image Classification and Semantic Segmentation**[[paper](https://arxiv.org/pdf/2404.10864)][[code](https://github.com/altndrr/vicss)]  
 [2024-Arxiv] **Vocabulary-free Image Classification and Semantic Segmentation**[[paper](https://arxiv.org/pdf/2404.10864)][[code](https://github.com/altndrr/vicss)]  
 [2024-ICML] **Image Clustering with External Guidance**[[paper](https://arxiv.org/pdf/2310.11989)][[code](https://github.com/XLearning-SCU/2024-ICML-TAC)]  

 [2025-Arxiv] **Clustering Properties of Self-Supervised Learning**[[paper](https://arxiv.org/pdf/2501.18452?#page=12.76)]  
 [2025-Arxiv] **Classifier-guided CLIP Distillation for Unsupervised Multi-label Classification**[[paper](https://arxiv.org/pdf/2503.16873)][[code](https://github.com/k0u-id/CCD)]  
 [2025-Arxiv] **Vocabulary-free few-shot learning for Vision-Language Models**[[paper](https://arxiv.org/pdf/2506.04005)][[code](https://github.com/MaxZanella/vocabulary-free-FSL)]  
 [2025-CVPR] **Exposure-slot: Exposure-centric representations learning with Slot-in-Slot Attention for Region-aware Exposure Correction**[[paper](https://cvpr.thecvf.com/virtual/2025/poster/33508)][[code](https://github.com/dgjung0220/Exposure-slot-official)]  
 [2025-ICLR] **Eagle: Exploring the design space for multimodal llms with mixture of encoders**[[paper](https://arxiv.org/pdf/2408.15998)]  
 [2025-ICML] **Towards a Unified Framework of Clustering-based Anomaly Detection**[[paper](https://arxiv.org/pdf/2406.00452)]  
 [2025-CVPR] **Distilling Spectral Graph for Object-Context Aware Open-Vocabulary Semantic Segmentation**[[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Kim_Distilling_Spectral_Graph_for_Object-Context_Aware_Open-Vocabulary_Semantic_Segmentation_CVPR_2025_paper.pdf)][[code](https://micv-yonsei.github.io/cass/)]  
 [2025-ICCV] **On the Provable Importance of Gradients for Language-Assisted Image Clustering**  

 [2026-ICLR] **Samples Are Notequal: A Sample Selection Approach for Deep Clustering**[[paper](https://openreview.net/attachment?id=SpsmpVo349&name=pdf)]  
 [2026-ICLR] **CLIP as a Prior Teacher: Breaking the Label Dependency in Semi-Supervised Learning**[[paper](https://openreview.net/attachment?id=e9JphzQ5Gr&name=pdf)]  
 [2026-ICLR] **Spatial Structure and Selective Text Jointly Facilitate Image Clustering**[[paper](https://openreview.net/attachment?id=3DOgmfZ2k6&name=pdf)]  
 [2026-ICLR] **Delving into Spectral Clustering with Vision-Language Representations**[[paper](https://openreview.net/attachment?id=s1ea8y8VUL&name=pdf)]  
 [2026-ICLR] **Infer: Embedding Integration with Feature Refinement for Few-Shot Learning in VLMs**[[paper](https://openreview.net/attachment?id=sD18KHrPbB&name=pdf)]  
 [2026-] **You Can Trust Your Clustering Model: A Parameter-free Self-Boosting Plug-in for Deep Clustering**[[paper](https://openreview.net/pdf?id=JqyEIr41M4)]  
 

### Semi-Supervised Learning
 [2023-NeurIPS] **S-CLIP: Semi-supervised Vision-Language Learning using Few Specialist Captions** [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/c06f788963f0ce069f5b2dbf83fe7822-Paper-Conference.pdf)] [[code](https://proceedings.neurips.cc/paper_files/paper/2023/file/c06f788963f0ce069f5b2dbf83fe7822-Paper-Conference.pdf)]  
 [2023-NeurIPS] **Vocabulary-free Image Classification**[[paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/619cbddb92b8c6fecaf2b86463153be9-Paper-Conference.pdf)][[code](https://github.com/altndrr/vic)]  

 [2024-Arxiv] **FINESSL:Erasing the Bias: Fine-Tuning Foundation Models for Semi-Supervised Learning** [[paper](https://arxiv.org/pdf/2405.11756)] [[code](https://github.com/Gank0078/FineSSL)]  
 [2024-NeurIPS] **OwMatch: Conditional Self-Labeling with Consistency for Open-World Semi-Supervised Learning** [[paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/b4fd162d3e2d015233486a2e313828a7-Paper-Conference.pdf)] [[code](https://github.com/niusj03/OwMatch)]  
 [2024-ICML] **Candidate Pseudolabel Learning: Enhancing Vision-Language Models by Prompt Tuning with Unlabeled Data**[[paper](https://arxiv.org/pdf/2406.10502)]  
 [2024-Arxiv] **Mixture-of-Subspaces in Low-Rank Adaptation**[[paper](https://arxiv.org/pdf/2406.11909)]  
 
 [2025-Arxiv] **Revisiting Semi-Supervised Learning in the Era of Foundation Models**[[paper](https://arxiv.org/pdf/2503.09707)] [[code](https://github.com/OSU-MLB/SSL-Foundation-Models)]  
 [2025-Arxiv] **CGMatch: A Different Perspective of Semi-supervised Learning** [[paper](https://arxiv.org/pdf/2503.02231?)] [[code](https://github.com/BoCheng-96/CGMatch)]  
 [2025-Arxiv] **FATE: A Prompt-Tuning-Based Semi-Supervised Learning Framework for Extremely Limited Labeled Data**[[paper](https://arxiv.org/pdf/2504.09828)][[code](https://anonymous.4open.science/r/Semi-supervised-learning-BA72)]  
 [2025-Arxiv] **SelfPrompt: Confidence-Aware Semi-Supervised Tuning for Robust Vision-Language Model Adaptatio**[[paper](https://arxiv.org/pdf/2501.14148)]  
 [2025-Arxiv] **FATE: A Prompt-Tuning-Based Semi-Supervised Learning Framework for Extremely Limited Labeled Data**[[paper](https://arxiv.org/pdf/2504.09828)][[code](https://anonymous.4open.science/r/Semi-supervised-learning-BA72)]  
 [2025-Arxiv] **Solving Semi-Supervised Few-Shot Learning from an Auto-Annotation Perspective**[[paper](https://arxiv.org/pdf/2512.10244)]  


 [2025-Arxiv] **Revisiting Semi-Supervised Learning in the Era of Foundation Models**[[paper](https://arxiv.org/pdf/2503.09707)][[code](https://github.com/OSU-MLB/SSL-Foundation-Models)]  
 [2025-CVPR] **Language-Assisted Debiasing and Smoothing for Foundation Model-Based Semi-Supervised Learning**[[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Zheng_Language-Assisted_Debiasing_and_Smoothing_for_Foundation_Model-Based_Semi-Supervised_Learning_CVPR_2025_paper.pdf)]   
 [2025-CVPR] **Towards Cost-Effective Learning: A Synergy of Semi-Supervised and Active Learning**[[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Yin_Towards_Cost-Effective_Learning_A_Synergy_of_Semi-Supervised_and_Active_Learning_CVPR_2025_paper.pdf)]  
 [2025-CVPR] **Seek Common Ground While Reserving Differences: Semi-supervised Image-Text Sentiment Recognition**[[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Xia_Seek_Common_Ground_While_Reserving_Differences_Semi-Supervised_Image-Text_Sentiment_Recognition_CVPR_2025_paper.pdf)]  
 [2025-CVPR] **CLIP-driven Coarse-to-fine Semantic Guidance for Fine-grained Open-set Semi-supervised Learning**[[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Li_CLIP-driven_Coarse-to-fine_Semantic_Guidance_for_Fine-grained_Open-set_Semi-supervised_Learning_CVPR_2025_paper.pdf)]  
 [2025-CVPR] **Learning Textual Prompts for Open-World Semi-Supervised Learning**[[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Fan_Learning_Textual_Prompts_for_Open-World_Semi-Supervised_Learning_CVPR_2025_paper.pdf)]  
 [2025-ICLR] **Semi-Supervised CLIP Adaptation by Enforcing Semantic and Trapezoidal Consistency**[[paper](https://openreview.net/pdf?id=97D725GJtQ)][[code](https://github.com/Gank0078/SemiCLIP)]  
 [2025-ICCV] **SemiVisBooster: Boosting Semi-Supervised Learning for Fine-Grained Classification through Pseudo-Label Semantic Guidance**[[paper]()]   

### GCD
 [2024-Arxiv] **SelEx: Self-Expertise in Fine-Grained Generalized Category Discovery**[[paper](https://arxiv.org/pdf/2408.14371)][[code](https://github.com/SarahRastegar/SelEx)]  
 [2024-CVPR] **Targeted Representation Alignment for Open-World Semi-Supervised Learning**[[paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Xiao_Targeted_Representation_Alignment_for_Open-World_Semi-Supervised_Learning_CVPR_2024_paper.pdf)][[code](https://github.com/Justherozen/TRAILER)]  
 [2024-ICJV]  **MutualFormer: Multi-modal Representation Learning via Cross-Diffusion Attention**[[paper](https://link.springer.com/article/10.1007/s11263-024-02067-x)][[code](https://github.com/SissiW/MutualFormer)]  

 [2025-Arxiv] **Category Discovery: An Open-World Perspective**[[paper](https://arxiv.org/pdf/2509.22542)]  
 [2025-Arxiv] **Learning Part Knowledge to Facilitate Category Understanding for Fine-Grained Generalized Category Discovery**[[paper](https://arxiv.org/pdf/2503.16782)]  
 [2025-Arxiv] **Hyperbolic Category Discovery**[[paper](https://visual-ai.github.io/hypcd/)][[code](https://github.com/Visual-AI/HypCD)]  
 [2025-CVPR] **GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery**[[paper](https://arxiv.org/pdf/2403.09974)][[code](https://github.com/enguangW/GET)]  
 [2025-CVPR] **Mamba-Adaptor: State Space Model Adaptor for Visual Recognition**[[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Xie_Mamba-Adaptor_State_Space_Model_Adaptor_for_Visual_Recognition_CVPR_2025_paper.pdf)]  
 [2025-CVPR] **Adaptive Part Learning for Fine-Grained Generalized Category Discovery: APlug-and-Play Enhancement**[[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Dai_Adaptive_Part_Learning_for_Fine-Grained_Generalized_Category_Discovery_A_Plug-and-Play_CVPR_2025_paper.pdf)]  
 [2025-CVPR] **Less Attention is More: Prompt Transformer for Generalized Category Discovery**[[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Less_Attention_is_More_Prompt_Transformer_for_Generalized_Category_Discovery_CVPR_2025_paper.pdf)][[code](https://github.com/wendy26zhang/AptGCD)]  
 [2025-ICLR] **Generalized Category Discovery Utilizing Reciprocal Learning and Class-Wise Distribution Regularization**[[paper](https://openreview.net/pdf?id=On8E0U9vbz)]  
 [2025-ICLR] **DEBGCD: Debiased Learning with Distribution Guidance for Generalized Category Discovery**[[paper](https://arxiv.org/pdf/2504.04804?)][[code](https://visual-ai.github.io/debgcd/)]]  
 [2025-CVPR] **Hyperbolic Category Discovery**[[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Hyperbolic_Category_Discovery_CVPR_2025_paper.pdf)][[code](https://visual-ai.github.io/hypcd/)]  
 [2025-ICCV] **Learning Separable Fine-Grained Representation via Dendrogram Construction from Coarse Labels for Fine-grained Visual Recognition**[[code](https://github.com/BeCarefulOfYournaoke/BuCSFR)]  
 [2025-ICCV] **AllGCD: Leveraging All Unlabeled Data for Generalized Category Discovery**  
 [2025-ICCV] **Leveraging Synthesized Novel Samples for On-the-Fly Fine-Grained Category Discovery**  
 [2025-CVPR] **MOS:Modeling Object-Scene Associations in Generalized Category Discovery**[[paper](https://arxiv.org/pdf/2503.12035)][[code](https://github.com/JethroPeng/MOS)]  

 [2026-ICLR] **Gestalt Generalized Category Discovery**[[paper](https://openreview.net/attachment?id=pbMzwCnGpq&name=pdf)]  
 [2026-ICLR] **Bures Generalized Category Discovery**[[paper](https://openreview.net/attachment?id=nfVKTJ1MJ3&name=pdf)]  
 [2026-ICLR] **PARTCO: Part-Level Correspondence Priors Enhance Category Discovery**[[paper](https://openreview.net/attachment?id=xiKtlg1jED&name=pdf)]  
 [2026-ICLR] **Adaptive Gaussian Expansion for On-the-Fly Category Discovery**[[paper](https://openreview.net/attachment?id=Y59JeAbM3j&name=pdf)]  
 [2026-ICLR] **Consistency and Unified Semantic Regularization for Generalized Category Discovery**[[paper](https://openreview.net/attachment?id=KmJfy0VtRJ&name=pdf)]  
 [2026-ICLR] **Multi-Modal Representation Learning via Semi-Supervised Rate Reduction for Generalized Category Discovery**[[paper](https://openreview.net/attachment?id=40TvCwYSau&name=pdf)]  
 [2026-ICLR] **SpectralGCD: Spectral Concept Selection and Cross-Modal Representation Learning for Generalized Category Discovery**[[paper](https://openreview.net/attachment?id=PyfV9tFmdR&name=pdf)]  

### Pruning
[2024-CVPR] **Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers**[[paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Zero-TPrune_Zero-Shot_Token_Pruning_through_Leveraging_of_the_Attention_Graph_CVPR_2024_paper.pdf)]  
[2025-ICCV] **METEOR: Multi-Encoder Collaborative Token Pruning for Efficient Vision Language Models**[[paper](https://iccv.thecvf.com/virtual/2025/poster/2161)]  
[2025-ICCV] **Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs**[[paper](https://arxiv.org/pdf/2412.01818)]  

### Model integration 
 [2024-Arxiv] **BRAVE :Broadening the visual encoding of vision-language models**[[paper](https://arxiv.org/pdf/2404.07204)]  
 [2024-Arxiv] **ProxyCLIP: Proxy Attention Improves CLIP for Open-Vocabulary Segmentation**[[paper](https://arxiv.org/pdf/2408.04883)][[code](https://github.com/mc-lan/ProxyCLIP)]  
 [2024-Arxiv] **Let Go of Your Labels with Unsupervised Transfer**[[paper](https://arxiv.org/pdf/2406.07236?)]  
 [2024-NeurIPS] **No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen Representations**[[paper](https://arxiv.org/pdf/2407.10964)]  

 [2025-Arxiv] **LEO: Boosting Mixture of Vision Encoders for Multimodal Large Language Models**[[paper](https://arxiv.org/pdf/2501.06986)][[code](https://github.com/Mozhgan91/LEO)]  
 [2025-Arxiv] **CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image Collections**[[paper](https://arxiv.org/pdf/2411.19346)][[code](https://github.com/fazliimam/NoLA)]  
 [2025-CVPR] **Wang DeCLIP Decoupled Learning for Open-Vocabulary Dense Perception**[[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_DeCLIP_Decoupled_Learning_for_Open-Vocabulary_Dense_Perception_CVPR_2025_paper.pdf)]  
 [2025-CVPR] **MMRL:Multi-Modal Representation Learning for Vision-Language Models**[[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Guo_MMRL_Multi-Modal_Representation_Learning_for_Vision-Language_Models_CVPR_2025_paper.pdf)][[code](https://github.com/yunncheng/MMRL)]  
 [2025-ICML] **Representation Surgery in Model Merging with Probabilistic Modeling**[[paper](https://openreview.net/pdf?id=a02CH43z1G)][[code]()]  
 [2025-ICCV] **Unknown Text Learning for CLIP-based Few-Shot Open-set Recognition**  
 [2025-ICCV] **Enhancing Few-Shot Vision-Language Classification with Large Multimodal Model Features**[[paper](https://arxiv.org/pdf/2412.00142)]  

 [2025-Arxiv] **GaussianToken: An Effective Image Tokenizer with 2D Gaussian Splatting**[[paper](https://arxiv.org/pdf/2501.15619)]  


### Fine-tuning of the base model
 [2025-CVPR] **COSMIC:Clique-Oriented Semantic Multi-space Integration for Robust CLIP Test-Time Adaptation**[[paper](https://arxiv.org/pdf/2503.23388)]  
 [2025-CVPR] **VTD-CLIP:Video-to-Text Discretization via Prompting CLIP**[[paper](https://arxiv.org/pdf/2503.18407#/)]  
 [2025-ICLR] **Attribute-Based Visual Reprogramming for Vision-Language Models**[[paper](https://arxiv.org/pdf/2501.13982)]  
 [2025-Arxiv] **Introducing Visual Perception Token into Multimodal Large Language Model**[[paper](https://arxiv.org/pdf/2502.17425)]  
 [2024-Arxiv] **SINDER: Repairing the Singular Defects of DINOv2**[[paper](https://arxiv.org/pdf/2407.16826)][[code](https://github.com/haoqiwang/sinder)]

### Zero-Shot
 [2026-ICLR] **Image-Free Zero-Shot Learning via Adaptive Semantic-Guided Classifier Injection**[[paper](https://openreview.net/attachment?id=dbgxBSRVfR&name=pdf)]  
 
### Preprint
 [2025-ICML] **Clustering via Self-Supervised Diffusion**[[paper](https://icml.cc/virtual/2025/poster/46196)]  
 [2025-ICML] **Super Deep Contrastive Information Bottleneck for Multi-modal Clustering**[[paper](https://icml.cc/virtual/2025/poster/46541)]  
 [2025-ICML] **A Peer-review Look on Multi-modal Clustering: An Information Bottleneck Realization Method**[[paper](https://icml.cc/virtual/2025/poster/46541)]  
 [2025-ICML] **Kernel-based Unsupervised Embedding Alignment for Enhanced Visual Representation in Vision-language Models**[[paper](https://icml.cc/virtual/2025/poster/46673)]  
 [2025-ICML] **Deep Unsupervised Hashing via External Guidance**[[paper](https://icml.cc/virtual/2025/poster/43790)]  
 [2025-ICML] **SEAD: Unsupervised Ensemble of Streaming Anomaly Detectors**[[paper](https://icml.cc/virtual/2025/poster/46199)]  

### Research group of interest
 Kai Han, The University of Hong Kong[[home](https://www.kaihan.org/)]  
 Xiangyong Cao, Xi’an Jiaotong University[[home](https://gr.xjtu.edu.cn/web/caoxiangyong)]
 
