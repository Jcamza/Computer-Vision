# Computer-Vision-paper
As a new postgraduate, this repository contains a collection of papers on computer vision and artificial intelligence to facilitate my learning and document my progress.

### Semi-Supervised Learning
 [2023-NeurIPS] **S-CLIP: Semi-supervised Vision-Language Learning using Few Specialist Captions** [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/c06f788963f0ce069f5b2dbf83fe7822-Paper-Conference.pdf)] [[code](https://proceedings.neurips.cc/paper_files/paper/2023/file/c06f788963f0ce069f5b2dbf83fe7822-Paper-Conference.pdf)]  

 [2024-Arxiv] **FINESSL:Erasing the Bias: Fine-Tuning Foundation Models for Semi-Supervised Learning** [[paper](https://arxiv.org/pdf/2405.11756)] [[code](https://github.com/Gank0078/FineSSL)]  
 [2024-NeurIPS] **OwMatch: Conditional Self-Labeling with Consistency forÂ Open-World Semi-Supervised Learning** [[paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/b4fd162d3e2d015233486a2e313828a7-Paper-Conference.pdf)] [[code](https://github.com/niusj03/OwMatch)]  

 [2025-Arxiv] **Revisiting Semi-Supervised Learning in the Era of Foundation Models**[[paper](https://arxiv.org/pdf/2503.09707)] [[code](https://github.com/OSU-MLB/SSL-Foundation-Models)]  
 [2025-Arxiv] **CGMatch: A Different Perspective of Semi-supervised Learning** [[paper](https://arxiv.org/pdf/2503.02231?)] [[code](https://github.com/BoCheng-96/CGMatch)]  
 [2025-Arxiv] **FATE: A Prompt-Tuning-Based Semi-Supervised Learning Framework for Extremely Limited Labeled Data**[[paper](https://arxiv.org/pdf/2504.09828)][[code](https://anonymous.4open.science/r/Semi-supervised-learning-BA72)]  
 [2025-Arxiv] **SelfPrompt: Confidence-Aware Semi-Supervised Tuning for Robust Vision-Language Model Adaptatio**[[paper](https://arxiv.org/pdf/2501.14148)]  
 [2025-CVPR] **Language-Assisted Debiasing and Smoothing for Foundation Model-Based Semi-Supervised Learning**  
 [2025-CVPR] **Towards Cost-Effective Learning: A Synergy of Semi-Supervised and Active Learning**  
 [2025-CVPR] **Seek Common Ground While Reserving Differences: Semi-supervised Image-Text Sentiment Recognition**  
 [2025-CVPR] **CLIP-driven Coarse-to-fine Semantic Guidance for Fine-grained Open-set Semi-supervised Learning**  
 [2025-CVPR] **Learning Textual Prompts for Open-World Semi-Supervised Learning**  
 [2025-ICLR] **SEMI-SUPERVISED CLIP ADAPTATION BY ENFORCING SEMANTIC AND TRAPEZOIDAL CONSISTENCY**[[paper](https://openreview.net/pdf?id=97D725GJtQ)][[code](https://github.com/Gank0078/SemiCLIP)]  

### Object-Centric Learning
 [2023-NIPS] **2023-nips-Object-centric Learning with Cyclic Walks between Parts and Whole**[[paper](https://arxiv.org/pdf/2302.08023)][[code](https://github.com/ZhangLab-DeepNeuroCogLab/Parts-Whole-Object-Centric-Learning/)]  
 
 [2025-Arxiv] **Are WeDonewith Object-Centric Learning?**[[paper](https://arxiv.org/pdf/2504.07092)][[code(https://github.com/AlexanderRubinstein/diverse-universe-public)]  
 [2025-Arxiv] **CTRL-O: Language-Controllable Object-Centric Visual Representation Learning**[[paper](https://arxiv.org/pdf/2503.21747)][[code](https://ctrl-o-paper.github.io/)  
 [2025-CVPR] **Pay Attention to the Foreground in Object-Centric Learning**[[paper](https://cvpr.thecvf.com/virtual/2025/poster/34483)]  
 

### NEWS
 [2023-ICCV] **Zero-Shot Composed Image Retrieval with Textual Inversion**[[paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Baldrati_Zero-Shot_Composed_Image_Retrieval_with_Textual_Inversion_ICCV_2023_paper.pdf)][[code](https://github.com/miccunifi/SEARLE.)]  
 
 [2024-ICML] **Image Clustering with External Guidance**[[paper](https://arxiv.org/pdf/2310.11989)][[code](https://github.com/XLearning-SCU/2024-ICML-TAC)]  
 
 [2025-CVPR] **GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery**[[paper](https://arxiv.org/pdf/2403.09974)][[code](https://github.com/enguangW/GET)]  
 [2025-CVPR] **COSMIC:Clique-Oriented Semantic Multi-space Integration for Robust CLIP Test-Time Adaptation**[[paper](https://arxiv.org/pdf/2503.23388)]  
 [2025-CVPR] **VTD-CLIP:Video-to-Text Discretization via Prompting CLIP**[[paper](https://arxiv.org/pdf/2503.18407#/)]  
